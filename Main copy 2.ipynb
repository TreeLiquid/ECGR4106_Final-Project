{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os, re, struct, socket\n",
    "\n",
    "import import_ipynb\n",
    "import project_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "#GPU Checking\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDS2017_Monday ==== No anomolies\n",
    "#IDS2017_data = 'data\\TrafficLabelling\\Monday-WorkingHours.pcap_ISCX.csv'\n",
    "#IDS2017_DF = pd.DataFrame(pd.read_csv(IDS2017_data))\n",
    "\n",
    "\n",
    "#IDS2017_features = IDS2017_DF.iloc[:, [6, 1, 3, 2, 4, 5, 7, 84]] # 85 Columns\n",
    "# Summing the total length of forward and backward packets\n",
    "#IDS2017_features['Total Length'] = IDS2017_DF.iloc[:, 11] + IDS2017_DF.iloc[:, 12]\n",
    "#IDS2017_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KDDcup Corrected File\n",
    "#KDD_fp = 'data\\kddcup\\kddcup_corrected.csv'\n",
    "#KDD_DF = pd.DataFrame(pd.read_csv(KDD_fp))\n",
    "#KDD_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCAP DATA\n",
    "##pcap_fp = 'data\\pcap\\midnight.csv'\n",
    "#pcap_DF = pd.DataFrame(pd.read_csv(pcap_fp))\n",
    "#pcap_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert IP addresses into integers\n",
    "def encode_ip(ip_address):\n",
    "    try:\n",
    "        return struct.unpack(\"!L\", socket.inet_aton(ip_address))[0]\n",
    "    except socket.error:\n",
    "        # Handle the case for invalid IP addresses\n",
    "        return 0\n",
    "\n",
    "\n",
    "#Info Extracter for Pcap type files\n",
    "def extract_network_info(info_str):\n",
    "    # Dictionary to hold the extracted values\n",
    "    extracted_info = {\n",
    "        'sending_port': None,\n",
    "        'receiving_port': None,\n",
    "        'length': None,\n",
    "        'seq': None,\n",
    "        'ack': None,\n",
    "        'win': None,\n",
    "        'tsval': None,\n",
    "        'tsecr': None,\n",
    "        'sle_sre': [],\n",
    "        'info_messages': [],\n",
    "    }\n",
    "    \n",
    "    # Check for and extract port and length information\n",
    "    match = re.search(r'(\\S+)\\s+\\d+\\s+(\\d+)\\s*>\\s*(\\d+)(?:.*Len=(\\d+))?', info_str)\n",
    "    if match:\n",
    "        # Protocol and port information\n",
    "        extracted_info['protocol'] = match.group(1)\n",
    "        extracted_info['sending_port'] = int(match.group(2))\n",
    "        extracted_info['receiving_port'] = int(match.group(3))\n",
    "        \n",
    "        # Length information if present\n",
    "        if match.group(4):\n",
    "            extracted_info['length'] = int(match.group(4))\n",
    "\n",
    "    # Extract other information if present\n",
    "    for key in ['seq', 'ack', 'win', 'tsval', 'tsecr']:\n",
    "        match = re.search(r'{}=(\\d+)'.format(key.capitalize()), info_str)\n",
    "        if match:\n",
    "            extracted_info[key] = int(match.group(1))\n",
    "\n",
    "    # Extract SLE and SRE pairs\n",
    "    extracted_info['sle_sre'] = re.findall(r'SLE=(\\d+)\\s*SRE=(\\d+)', info_str)\n",
    "    \n",
    "    # Capture informational messages like \"Ignored Unknown Record\" or \"Continuation Data\"\n",
    "    info_msg_match = re.search(r'\\[.*?\\]', info_str)\n",
    "    if info_msg_match:\n",
    "        extracted_info['info_messages'].append(info_msg_match.group(0))\n",
    "\n",
    "    return extracted_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Dataset\n",
    "class NetworkDataset(Dataset):\n",
    "    def __init__(self, directory_path, num_entries=1000):\n",
    "    # Load and concatenate all CSV files\n",
    "        self.data = pd.concat(\n",
    "            [pd.read_csv(os.path.join(directory_path, file)) for file in os.listdir(directory_path) if file.endswith('.csv')],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        # Limit the data to the first num_entries\n",
    "        self.data = self.data.iloc[:num_entries]\n",
    "\n",
    "        # Sort data by 'Time' if necessary\n",
    "        #self.data.sort_values(by='Time', inplace=True)\n",
    "        \n",
    "        # Preprocess data (fill missing values, extract features, encode data)\n",
    "        self.data.fillna(self.get_default_fill_values(), inplace=True)\n",
    "        self.extract_and_encode_features()\n",
    "\n",
    "        #Scaling\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scale_features()\n",
    "\n",
    "    def get_default_fill_values(self):\n",
    "        # Default fill values for missing data\n",
    "        return {\n",
    "            'latency': 0,\n",
    "            'traffic_type': 'unknown',\n",
    "            'anomaly_label': 0,\n",
    "            'bottleneck': 'no',\n",
    "            'Length': 0,\n",
    "            'Source': '0.0.0.0',\n",
    "            'Destination': '0.0.0.0',\n",
    "            'Protocol': 'unknown',\n",
    "            'Time': self.data['Time'].median() if not self.data['Time'].isna().all() else 0,\n",
    "            'Info': 'No Information'\n",
    "        }\n",
    "\n",
    "    def extract_and_encode_features(self):\n",
    "        # Extract and encode features\n",
    "        self.data['Info'] = self.data['Info'].astype(str)\n",
    "        extracted_info = self.data['Info'].apply(extract_network_info).apply(pd.Series)\n",
    "        self.data = pd.concat([self.data, extracted_info], axis=1)\n",
    "        self.data['source_encoded'] = self.data['Source'].apply(encode_ip)\n",
    "        self.data['destination_encoded'] = self.data['Destination'].apply(encode_ip)\n",
    "        self.data['Protocol'] = pd.factorize(self.data['Protocol'])[0]\n",
    "        self.data['Time'] = self.data['Time'].astype(float)\n",
    "        self.data['Length'] = self.data['Length'].astype(float)\n",
    "\n",
    "        # Precompute BW & Throughput\n",
    "        self.data['Time_diff'] = self.data['Time'].diff().fillna(0).clip(lower=1e-5)\n",
    "        self.data['Bandwidth'] = self.data['Length'] / self.data['Time_diff']\n",
    "        self.data['Throughput'] = self.data['Bandwidth']  # Simplified, adjust if different logic needed\n",
    "        self.data['Latency'] = self.data['Length'] / 1000  # Assuming Length is in bytes and Latency is in ms\n",
    "\n",
    "        # Type Conversions\n",
    "        self.data['Time'] = self.data['Time'].astype(float)\n",
    "        self.data['Length'] = self.data['Length'].astype(float)\n",
    "        self.data['Bandwidth'] = self.data['Bandwidth'].astype(float)\n",
    "        self.data['Throughput'] = self.data['Throughput'].astype(float)\n",
    "        self.data['Latency'] = self.data['Latency'].astype(float)\n",
    "\n",
    "    def scale_features(self):\n",
    "        # Scale model input feautres\n",
    "        features_to_scale = ['Time', 'Length', 'Time_diff', 'Bandwidth', 'Throughput', 'Latency']\n",
    "        self.data[features_to_scale] = self.scaler.fit_transform(self.data[features_to_scale])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        features = torch.tensor([row['Time'], row['source_encoded'], row['destination_encoded'], row['Protocol'], row['Length']], dtype=torch.float32)\n",
    "\n",
    "        ## Reg Features\n",
    "        bandwidth = torch.tensor([row['Bandwidth']], dtype=torch.float32)\n",
    "        throughput = torch.tensor([row['Throughput']], dtype=torch.float32)\n",
    "        latency = torch.tensor([row['Latency']], dtype=torch.float32)\n",
    "        # network_flow = Client -> Port -> Switch -> Master Swtich -> Firewall -> Inline Network Encryter -> Router -> Outbound                 \n",
    "        # resource_usage = no RAM/CPU data available \n",
    "\n",
    "\n",
    "        ## Classification targets\n",
    "        #traffic_type = torch.tensor(pd.factorize(data['traffic_type'])[0], dtype=torch.long)\n",
    "        #anomaly_detection = torch.tensor(data['anomaly_label'].values, dtype=torch.long)\n",
    "        #network_resource_status = torch.tensor(pd.factorize(data['resource_status'])[0], dtype=torch.long)\n",
    "        #bottlenecks = torch.tensor(pd.factorize(data['bottleneck'])[0], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "    #'features': features,\n",
    "    'bandwidth': bandwidth,\n",
    "    #'throughput': throughput,\n",
    "    #'latency': latency,\n",
    "    #'traffic_type': traffic_type,\n",
    "    #'anomaly_detection': anomaly_detection,\n",
    "    #'network_resource_status': network_resource_status,\n",
    "    #'bottlenecks': bottlenecks,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the Dataset\n",
    "batch_size = 256\n",
    "dataset = NetworkDataset('data\\pcap')\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training and Test set splits\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0377],\n",
      "        [ 1.0323],\n",
      "        [ 0.5246],\n",
      "        [ 0.9685],\n",
      "        [ 0.9791],\n",
      "        [-0.2926],\n",
      "        [ 1.0430],\n",
      "        [ 1.0377],\n",
      "        [ 1.0323],\n",
      "        [-0.5033],\n",
      "        [ 1.0430],\n",
      "        [-0.4609],\n",
      "        [ 0.7636],\n",
      "        [-0.3624],\n",
      "        [ 0.7690],\n",
      "        [-0.9498],\n",
      "        [-0.8512],\n",
      "        [-0.8752],\n",
      "        [-0.8459],\n",
      "        [ 0.2380],\n",
      "        [ 2.8574],\n",
      "        [-0.9371],\n",
      "        [-0.8672],\n",
      "        [-0.8365],\n",
      "        [ 0.7636],\n",
      "        [ 0.7743],\n",
      "        [-0.7220],\n",
      "        [ 0.7636],\n",
      "        [-0.6757],\n",
      "        [ 0.9951],\n",
      "        [-0.5118],\n",
      "        [ 0.9951],\n",
      "        [ 1.0057],\n",
      "        [-0.8454],\n",
      "        [ 0.9685],\n",
      "        [-0.8446],\n",
      "        [-0.8552],\n",
      "        [-0.8552],\n",
      "        [-0.8499],\n",
      "        [-0.8446],\n",
      "        [-0.8552],\n",
      "        [-0.8552],\n",
      "        [-0.8499],\n",
      "        [-0.9365],\n",
      "        [-0.8752],\n",
      "        [-0.9141],\n",
      "        [-0.8832],\n",
      "        [-0.8779],\n",
      "        [-0.0721],\n",
      "        [ 1.1587],\n",
      "        [-0.4955],\n",
      "        [ 1.0057],\n",
      "        [-0.6930],\n",
      "        [-0.8034],\n",
      "        [ 0.9951],\n",
      "        [-0.9359],\n",
      "        [-0.8566],\n",
      "        [-0.8379],\n",
      "        [-0.8566],\n",
      "        [-0.8512],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8619],\n",
      "        [-0.8512],\n",
      "        [-0.8459],\n",
      "        [-0.8672],\n",
      "        [-0.8486],\n",
      "        [-0.8619],\n",
      "        [-0.8486],\n",
      "        [-0.8433],\n",
      "        [-0.6764],\n",
      "        [ 0.9951],\n",
      "        [ 1.0057],\n",
      "        [-0.8071],\n",
      "        [-0.2367],\n",
      "        [ 0.2914],\n",
      "        [-0.8406],\n",
      "        [-0.8512],\n",
      "        [-0.8506],\n",
      "        [-0.8300],\n",
      "        [-0.8512],\n",
      "        [-0.8459],\n",
      "        [-0.8300],\n",
      "        [-0.8246],\n",
      "        [ 0.2914],\n",
      "        [ 0.2967],\n",
      "        [-0.2473],\n",
      "        [-0.7865],\n",
      "        [-0.2420],\n",
      "        [ 0.7690],\n",
      "        [ 1.0004],\n",
      "        [ 0.7636],\n",
      "        [ 0.9951],\n",
      "        [-0.4006],\n",
      "        [ 0.7743],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.5911],\n",
      "        [ 0.9685],\n",
      "        [ 0.7636],\n",
      "        [ 0.7743],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.4640],\n",
      "        [ 0.7743],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.5575],\n",
      "        [ 1.0430],\n",
      "        [ 0.6074],\n",
      "        [ 0.7743],\n",
      "        [-0.9492],\n",
      "        [-0.8619],\n",
      "        [-0.8619],\n",
      "        [-0.8512],\n",
      "        [-0.7028],\n",
      "        [ 1.0323],\n",
      "        [-0.8619],\n",
      "        [-0.8672],\n",
      "        [ 1.0377],\n",
      "        [ 1.0323],\n",
      "        [-0.8175],\n",
      "        [-0.8619],\n",
      "        [ 0.7636],\n",
      "        [-0.8672],\n",
      "        [ 0.7743],\n",
      "        [-0.9401],\n",
      "        [-0.8566],\n",
      "        [-0.8971],\n",
      "        [-0.8512],\n",
      "        [ 1.0004],\n",
      "        [ 0.9951],\n",
      "        [-0.5458],\n",
      "        [ 0.7636],\n",
      "        [ 0.7743],\n",
      "        [ 1.0004],\n",
      "        [ 0.9951],\n",
      "        [-0.8552],\n",
      "        [ 0.9951],\n",
      "        [ 2.9319],\n",
      "        [ 2.9319],\n",
      "        [ 1.0430],\n",
      "        [ 1.0430],\n",
      "        [-0.9444],\n",
      "        [-0.8672],\n",
      "        [ 0.8178],\n",
      "        [ 0.9951],\n",
      "        [ 1.0057],\n",
      "        [-0.8858],\n",
      "        [-0.8672],\n",
      "        [-0.8619],\n",
      "        [-0.5639],\n",
      "        [ 0.9951],\n",
      "        [-0.7528],\n",
      "        [ 0.7636],\n",
      "        [ 0.7743],\n",
      "        [-0.7229],\n",
      "        [ 0.7397],\n",
      "        [ 0.7503],\n",
      "        [-0.7229],\n",
      "        [ 0.7397],\n",
      "        [ 0.7503],\n",
      "        [-0.9439],\n",
      "        [ 0.9738],\n",
      "        [ 0.9685],\n",
      "        [-0.9284],\n",
      "        [ 1.0377],\n",
      "        [ 1.0377],\n",
      "        [-0.8300],\n",
      "        [ 1.0323],\n",
      "        [ 1.0323],\n",
      "        [-0.8060],\n",
      "        [-0.8167],\n",
      "        [-0.9348],\n",
      "        [-0.8246],\n",
      "        [-0.8167],\n",
      "        [-0.8113],\n",
      "        [-0.8486],\n",
      "        [-0.9443],\n",
      "        [-0.8433],\n",
      "        [-0.2654],\n",
      "        [ 0.7636],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.9242],\n",
      "        [-0.8672],\n",
      "        [-0.8858],\n",
      "        [-0.8672],\n",
      "        [-0.8619],\n",
      "        [-0.8619],\n",
      "        [-0.8672],\n",
      "        [-0.9413],\n",
      "        [-0.8619],\n",
      "        [ 0.7636],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [ 0.7636],\n",
      "        [ 0.7743],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.9474],\n",
      "        [-0.8672],\n",
      "        [-0.8672],\n",
      "        [-0.8619],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.8680],\n",
      "        [-0.8526],\n",
      "        [-0.8526],\n",
      "        [-0.8473],\n",
      "        [ 0.6074],\n",
      "        [ 0.7743],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.6822],\n",
      "        [ 0.7743],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.9413],\n",
      "        [-0.8672],\n",
      "        [-0.8858],\n",
      "        [-0.8672],\n",
      "        [-0.8619],\n",
      "        [ 0.7636],\n",
      "        [ 0.7743],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.8118],\n",
      "        [ 0.7743],\n",
      "        [ 0.7690],\n",
      "        [ 0.7636],\n",
      "        [-0.7640],\n",
      "        [ 0.7636],\n",
      "        [ 0.7743],\n",
      "        [-0.7935],\n",
      "        [ 1.0589],\n",
      "        [-0.8717],\n",
      "        [ 0.9951],\n",
      "        [ 1.0057],\n",
      "        [ 0.9685],\n",
      "        [ 0.9685],\n",
      "        [ 0.9791],\n",
      "        [-0.5875],\n",
      "        [-0.3724],\n",
      "        [ 0.9738],\n",
      "        [ 0.9685],\n",
      "        [-0.8469],\n",
      "        [ 0.7636],\n",
      "        [ 0.7743],\n",
      "        [-0.5644],\n",
      "        [ 0.7636],\n",
      "        [ 0.7743],\n",
      "        [-0.9498],\n",
      "        [-0.8433],\n",
      "        [-0.8193]])\n"
     ]
    }
   ],
   "source": [
    "for data in loader:\n",
    "    #features = data['features']\n",
    "    bandwidth = data['bandwidth']\n",
    "    #throughput = data['throughput']\n",
    "    #latency = data['latency']\n",
    "\n",
    "    print( bandwidth,)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "dropout = 0.2\n",
    "bidirectional = False\n",
    "\n",
    "epochs = 10\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m project_models\u001b[38;5;241m.\u001b[39mntwkPETransformer(input_size, hidden_size, output_size, num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, nhead \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m Trans_tloss, Trans_vloss, Trans_vacc \u001b[38;5;241m=\u001b[39m \u001b[43mproject_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:7\u001b[0m, in \u001b[0;36mtrainTransformer\u001b[1;34m(model, epochs, criterion, optimizer, train_loader, test_loader, device)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#Transformer\n",
    "model = project_models.ntwkPETransformer(input_size, hidden_size, output_size, num_layers = 4, nhead = 2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "Trans_tloss, Trans_vloss, Trans_vacc = project_models.trainTransformer(model,epochs, criterion, optimizer, train_loader, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
