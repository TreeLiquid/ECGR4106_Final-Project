{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os, re, struct, socket\n",
    "\n",
    "import import_ipynb\n",
    "import project_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "#GPU Checking\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Flow ID', ' Source IP', ' Source Port', ' Destination IP',\n",
      "       ' Destination Port', ' Protocol', ' Timestamp', ' Flow Duration',\n",
      "       ' Total Fwd Packets', ' Total Backward Packets',\n",
      "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',\n",
      "       ' Fwd Packet Length Max', ' Fwd Packet Length Min',\n",
      "       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',\n",
      "       'Bwd Packet Length Max', ' Bwd Packet Length Min',\n",
      "       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',\n",
      "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
      "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
      "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
      "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',\n",
      "       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',\n",
      "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
      "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
      "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
      "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
      "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
      "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
      "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
      "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',\n",
      "       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',\n",
      "       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',\n",
      "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
      "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
      "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
      "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
      "       ' Idle Max', ' Idle Min', ' Label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Source IP</th>\n",
       "      <th>Source Port</th>\n",
       "      <th>Destination IP</th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>...</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.168.10.5-8.254.250.126-49188-80-6</td>\n",
       "      <td>8.254.250.126</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.5</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>03/07/2017 08:55:58</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.168.10.5-8.254.250.126-49188-80-6</td>\n",
       "      <td>8.254.250.126</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.5</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>03/07/2017 08:55:58</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192.168.10.5-8.254.250.126-49188-80-6</td>\n",
       "      <td>8.254.250.126</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.5</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>03/07/2017 08:55:58</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192.168.10.5-8.254.250.126-49188-80-6</td>\n",
       "      <td>8.254.250.126</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.5</td>\n",
       "      <td>49188</td>\n",
       "      <td>6</td>\n",
       "      <td>03/07/2017 08:55:58</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192.168.10.14-8.253.185.121-49486-80-6</td>\n",
       "      <td>8.253.185.121</td>\n",
       "      <td>80</td>\n",
       "      <td>192.168.10.14</td>\n",
       "      <td>49486</td>\n",
       "      <td>6</td>\n",
       "      <td>03/07/2017 08:56:22</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Flow ID      Source IP   Source Port  \\\n",
       "0   192.168.10.5-8.254.250.126-49188-80-6  8.254.250.126            80   \n",
       "1   192.168.10.5-8.254.250.126-49188-80-6  8.254.250.126            80   \n",
       "2   192.168.10.5-8.254.250.126-49188-80-6  8.254.250.126            80   \n",
       "3   192.168.10.5-8.254.250.126-49188-80-6  8.254.250.126            80   \n",
       "4  192.168.10.14-8.253.185.121-49486-80-6  8.253.185.121            80   \n",
       "\n",
       "   Destination IP   Destination Port   Protocol            Timestamp  \\\n",
       "0    192.168.10.5              49188          6  03/07/2017 08:55:58   \n",
       "1    192.168.10.5              49188          6  03/07/2017 08:55:58   \n",
       "2    192.168.10.5              49188          6  03/07/2017 08:55:58   \n",
       "3    192.168.10.5              49188          6  03/07/2017 08:55:58   \n",
       "4   192.168.10.14              49486          6  03/07/2017 08:56:22   \n",
       "\n",
       "    Flow Duration   Total Fwd Packets   Total Backward Packets  ...  \\\n",
       "0               4                   2                        0  ...   \n",
       "1               1                   2                        0  ...   \n",
       "2               1                   2                        0  ...   \n",
       "3               1                   2                        0  ...   \n",
       "4               3                   2                        0  ...   \n",
       "\n",
       "    min_seg_size_forward  Active Mean   Active Std   Active Max   Active Min  \\\n",
       "0                     20          0.0          0.0          0.0          0.0   \n",
       "1                     20          0.0          0.0          0.0          0.0   \n",
       "2                     20          0.0          0.0          0.0          0.0   \n",
       "3                     20          0.0          0.0          0.0          0.0   \n",
       "4                     20          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
       "0        0.0        0.0        0.0        0.0  BENIGN  \n",
       "1        0.0        0.0        0.0        0.0  BENIGN  \n",
       "2        0.0        0.0        0.0        0.0  BENIGN  \n",
       "3        0.0        0.0        0.0        0.0  BENIGN  \n",
       "4        0.0        0.0        0.0        0.0  BENIGN  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IDS2017_Monday ==== No anomolies\n",
    "IDS2017_data = 'data\\TrafficLabelling\\Monday-WorkingHours.pcap_ISCX.csv'\n",
    "IDS2017_DF = pd.DataFrame(pd.read_csv(IDS2017_data))\n",
    "\n",
    "\n",
    "\n",
    "print(IDS2017_DF.columns)\n",
    "IDS2017_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KDDcup Corrected File\n",
    "#KDD_fp = 'data\\kddcup\\kddcup_corrected.csv'\n",
    "#KDD_DF = pd.DataFrame(pd.read_csv(KDD_fp))\n",
    "#KDD_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert IP addresses into integers\n",
    "def encode_ip(ip_address):\n",
    "    try:\n",
    "        return struct.unpack(\"!L\", socket.inet_aton(ip_address))[0]\n",
    "    except socket.error:\n",
    "        # Handle the case for invalid IP addresses\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataset(Dataset):\n",
    "    def __init__(self, directory_path, num_entries=1000):\n",
    "        self.scaler = StandardScaler()  # Initialize the scaler once\n",
    "        self.data = self.load_and_process_data(directory_path, num_entries)\n",
    "\n",
    "    def load_and_process_data(self, file_path, num_entries):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if num_entries is not None:\n",
    "            df = df.iloc[:num_entries]  # Limit entries if needed\n",
    "        df = self.preprocess_data(df)  # Apply preprocessing\n",
    "        return df\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        # Encode features and scale\n",
    "        df = self.extract_and_encode_features(df)\n",
    "        df = self.scale_features(df)\n",
    "        return df\n",
    "\n",
    "    def scale_features(self, df):\n",
    "        # Scale model input features\n",
    "        features_to_scale = ['Time_diff', 'Bandwidth', 'Throughput', 'Latency', 'Flow Duration',\n",
    "                             'Fwd_to_Bwd_Packets', 'Fwd_to_Bwd_Bytes',\n",
    "                             'Rolling_Mean_Packet_Size', 'Rolling_Std_Packet_Size','Total Length']\n",
    "        df[features_to_scale] = self.scaler.fit_transform(df[features_to_scale])\n",
    "        return df\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        return self.data\n",
    "\n",
    "    def extract_and_encode_features(self, df):\n",
    "        df.columns = df.columns.str.lstrip()\n",
    "\n",
    "        # Extract and encode features\n",
    "        df['source_encoded'] = df['Source IP'].apply(encode_ip)\n",
    "        df['destination_encoded'] = df['Destination IP'].apply(encode_ip)\n",
    "        df = pd.get_dummies(df, columns=['Protocol'], prefix='Protocol')\n",
    "\n",
    "        # Time\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        df['Hour_of_Day'] = pd.to_datetime(df['Timestamp']).dt.hour\n",
    "        df['Is_Weekend'] = (pd.to_datetime(df['Timestamp']).dt.weekday >= 5).astype(int)\n",
    "\n",
    "        # Behaviors\n",
    "        df['Session Duration'] = df.groupby('Flow ID')['Timestamp'].transform(lambda x: x.max() - x.min())\n",
    "        df['Unique Source Ports'] = df.groupby('Flow ID')['Source Port'].transform('nunique')\n",
    "        df['Unique Destination Ports'] = df.groupby('Flow ID')['Destination Port'].transform('nunique')\n",
    "\n",
    "        # Ratios\n",
    "        df['Fwd_to_Bwd_Packets'] = df['Total Fwd Packets'] / (df['Total Backward Packets'] + 0.001)\n",
    "        df['Fwd_to_Bwd_Bytes'] = df['Total Length of Fwd Packets'] / (df['Total Length of Bwd Packets'] + 0.001)\n",
    "\n",
    "        # Rolling Window Statistics\n",
    "        window_size = 5  # Define the window size\n",
    "        df['Rolling_Mean_Packet_Size'] = df['Packet Length Mean'].rolling(window=window_size).mean()\n",
    "        df['Rolling_Std_Packet_Size'] = df['Packet Length Std'].rolling(window=window_size).std()\n",
    "        df['EMA_Packet_Size'] = df['Packet Length Mean'].ewm(span=window_size).mean()\n",
    "\n",
    "        # Precompute BW & Throughput\n",
    "        df['Time_diff'] = (df['Timestamp'] - df['Timestamp'].shift()).dt.total_seconds().fillna(0).clip(lower=1e-5)\n",
    "        df['Total Length'] = df['Total Length of Fwd Packets'] + df['Total Length of Bwd Packets']\n",
    "        df['Bandwidth'] = df['Total Length'] / df['Time_diff']\n",
    "        df['Throughput'] = df['Bandwidth']  # Simplified, adjust if different logic needed\n",
    "        df['Latency'] = df['Total Length'] / 1000  # Assuming Length is in bytes and Latency is in ms\n",
    "        float_cols = ['Time_diff', 'Bandwidth', 'Throughput', 'Latency', 'Flow Duration', 'Fwd_to_Bwd_Packets',\n",
    "                      'Fwd_to_Bwd_Bytes', 'Rolling_Mean_Packet_Size', 'Rolling_Std_Packet_Size']\n",
    "        df[float_cols] = df[float_cols].astype(float)\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        features = [\n",
    "            row['Time_diff'],\n",
    "            row['source_encoded'],\n",
    "            row['destination_encoded'],\n",
    "            row['Total Length'],\n",
    "            row['Hour_of_Day'],\n",
    "            row['Is_Weekend'],\n",
    "            row['Fwd_to_Bwd_Packets'],\n",
    "            row['Fwd_to_Bwd_Bytes'],\n",
    "            row['Rolling_Mean_Packet_Size'],\n",
    "            row['Rolling_Std_Packet_Size'],\n",
    "            row['EMA_Packet_Size']\n",
    "        ] + [row[col] for col in self.data.columns if col.startswith('Protocol_')]\n",
    "\n",
    "        features = torch.tensor(features, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "        bandwidth = torch.tensor([row['Bandwidth']], dtype=torch.float32)\n",
    "\n",
    "        return features, bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inout_sequences(df, seq_length):\n",
    "    sequences = []\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "    for i in range(len(df) - seq_length):\n",
    "        seq_features = []\n",
    "\n",
    "        for j in range(seq_length):\n",
    "            # Get numeric features for each timestep in the sequence\n",
    "            numeric_values = df.iloc[i + j][numeric_cols].values\n",
    "            numeric_tensor = torch.tensor(numeric_values, dtype=torch.float32)\n",
    "\n",
    "            # Get non-numeric features for each timestep in the sequence\n",
    "            non_numeric_values = df.iloc[i + j].drop(numeric_cols, axis=1).values\n",
    "\n",
    "            # Combine numeric and non-numeric features\n",
    "            feature_values = torch.cat((numeric_tensor, torch.tensor(non_numeric_values)), dim=0)\n",
    "\n",
    "            seq_features.append(feature_values)\n",
    "\n",
    "        seq_label = df.iloc[i + seq_length]['Bandwidth']  # Extract bandwidth value\n",
    "        sequences.append((seq_features, seq_label))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    features = []\n",
    "    bandwidths = []\n",
    "    for sample in batch:\n",
    "        seq_features, seq_label = sample\n",
    "        features.append(torch.stack(seq_features))\n",
    "        bandwidths.append(seq_label)\n",
    "    features = torch.stack(features)\n",
    "    bandwidths = torch.tensor(bandwidths)\n",
    "    return features, bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'get_dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[186], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get the data from the dataset\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataframe\u001b[49m()\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m      9\u001b[0m seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'get_dataframe'"
     ]
    }
   ],
   "source": [
    "## Using the Dataset\n",
    "batch_size = 200\n",
    "dataset = NetworkDataset('data\\Tempdir\\Monday-WorkingHours.pcap_ISCX.csv', num_entries= 1000)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Get the data from the dataset\n",
    "data = dataset.get_dataframe()\n",
    "df = pd.DataFrame(data)\n",
    "seq_length = 50\n",
    "\n",
    "print(df.head())\n",
    "print(df.keys())\n",
    "\n",
    "# Create sequences\n",
    "train_data = create_inout_sequences(df, seq_length)\n",
    "\n",
    "# Training and Test set splits\n",
    "train_size = int(len(train_data) * 0.8)\n",
    "test_size = len(train_data) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(train_data, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last= True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "print(len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Unpack the tuple\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[150], line 17\u001b[0m, in \u001b[0;36mcustom_collate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m     16\u001b[0m     seq_features, seq_label \u001b[38;5;241m=\u001b[39m sample\n\u001b[1;32m---> 17\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_features\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m     bandwidths\u001b[38;5;241m.\u001b[39mappend(seq_label)\n\u001b[0;32m     19\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(features)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    features, bandwidth = data  # Unpack the tuple\n",
    "    print(features, bandwidth)\n",
    "    if i == 1:  # Just check the first couple of batches\n",
    "        break\n",
    "\n",
    "for i, data in enumerate(test_loader):\n",
    "    print(f\"Batch {i+1} from test_loader:\")\n",
    "    print(data)  # Print out the batch\n",
    "    if i == 0:  # Just check the first batch\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 14\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "dropout = 0\n",
    "bidirectional = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m rnn_tloss, rnn_vloss \u001b[38;5;241m=\u001b[39m \u001b[43mproject_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_RNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:8\u001b[0m, in \u001b[0;36mtrain_RNN\u001b[1;34m(epochs, model, criterion, optimizer, train_loader, test_loader, device)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[150], line 17\u001b[0m, in \u001b[0;36mcustom_collate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m     16\u001b[0m     seq_features, seq_label \u001b[38;5;241m=\u001b[39m sample\n\u001b[1;32m---> 17\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_features\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m     bandwidths\u001b[38;5;241m.\u001b[39mappend(seq_label)\n\u001b[0;32m     19\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(features)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "model = project_models.ntwkRNN(input_size, hidden_size, output_size, dropout, bidirectional).to(device)\n",
    "\n",
    "epochs = 10\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "rnn_tloss, rnn_vloss = project_models.train_RNN(epochs, model, criterion, optimizer, train_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m project_models\u001b[38;5;241m.\u001b[39mntwkPETransformer(input_size, hidden_size, output_size, num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, nhead \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m Trans_tloss, Trans_vloss, Trans_vacc \u001b[38;5;241m=\u001b[39m \u001b[43mproject_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:7\u001b[0m, in \u001b[0;36mtrainTransformer\u001b[1;34m(model, epochs, criterion, optimizer, train_loader, test_loader, device)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#Transformer\n",
    "model = project_models.ntwkPETransformer(input_size, hidden_size, output_size, num_layers = 4, nhead = 2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "Trans_tloss, Trans_vloss, Trans_vacc = project_models.trainTransformer(model,epochs, criterion, optimizer, train_loader, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
